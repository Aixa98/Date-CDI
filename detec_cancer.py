# -*- coding: utf-8 -*-
"""Detec-Cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G5djjBf1kd2hM7SdMFlfBOdoDD95dzcp
"""

#Descripción: Este programa detecta el cancer de mama a travez de intelligencia artifical.

#Librerias python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#Carga de datos - Leer 100 datos de cabecera. 
from google.colab import files 
uploaded = files.upload()
df = pd.read_csv('data.csv') 
df.head(100)

#Numero de filas y columnas del dataset 
df.shape

#Buscamos columnas vacias
df.isna().sum() 
#Al realizar el analisis obtenemos que la columna 32 esta vacia

#Borramos la columna 32 ya que no es util
df = df.dropna(axis=1)

df.shape

"""
  El valor a analizar sera la columna diagnostico, 
  esta indica que el cancer es maligno con el campo "M" 
  e indica que es benigno con el campo "B". 
  Por esta razon realizamos un conteo de resultados arrojados en el dataset 

  El resultado del dataset es el siguiente: 
  Beningno B = 357
  Maligno  M = 212
"""
df['diagnosis'].value_counts()

#Visualizar grafico
sns.set_style("darkgrid", {"axes.facecolor": ".9"})
sns.countplot(df['diagnosis'], label="Cantidad")

#Tipos de datos
df.dtypes

#Codificar los datos categoricos, cambiar "M" y "B" a 1 y 0
from sklearn.preprocessing import LabelEncoder
labelencoder_Y = LabelEncoder()
df.iloc[:,1] = labelencoder_Y.fit_transform(df.iloc[:,1].values)

print(labelencoder_Y.fit_transform(df.iloc[:,1].values))

"""La ejecución de los graficos tarda 10min. aprox - hacer zoom para apreciar con detalle uno por uno """

sns.pairplot(df, hue="diagnosis")

df.head(5)

#Correlación entre columnas 
df.corr()

#Mapa de calor para visualizar correlación
plt.figure(figsize=(20,20))  
sns.heatmap(df.corr(), annot=True, fmt='.0%')
plt.gcf().set_size_inches(40, 20)

"""
  Explorar y limpiar los datos. Dividimos el conjunto de datos en: 
  Conjunto de datos independientes (x)
  y
  Conjunto de datos dependientes (y), datos del destino.
"""
X = df.iloc[:, 2:31].values 
Y = df.iloc[:, 1].values

#Preparamos el data set para entrenamiento y prueba. 
#Dividimos 75% para entrenamiento y 25% para pruebas. 
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)

"""
  Escalar los datos, para llevar todas las caracteristicas al mismo nivel de magnitud, 
  lo que significa que caracteristica / datos independientes estaran dentro de un rango especifico, 
  por ejemplo: 0 - 100 o 0 - 1
"""
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#Crear función para diferentes modelos 
def models(X_train,Y_train):
  
  #Using Logistic Regression 
  from sklearn.linear_model import LogisticRegression
  log = LogisticRegression(random_state = 0)
  log.fit(X_train, Y_train)
  
  #Using KNeighborsClassifier 
  from sklearn.neighbors import KNeighborsClassifier
  knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)
  knn.fit(X_train, Y_train)

  #Using SVC linear
  from sklearn.svm import SVC
  svc_lin = SVC(kernel = 'linear', random_state = 0)
  svc_lin.fit(X_train, Y_train)

  #Using SVC rbf
  from sklearn.svm import SVC
  svc_rbf = SVC(kernel = 'rbf', random_state = 0)
  svc_rbf.fit(X_train, Y_train)

  #Using GaussianNB 
  from sklearn.naive_bayes import GaussianNB
  gauss = GaussianNB()
  gauss.fit(X_train, Y_train)

  #Using DecisionTreeClassifier 
  from sklearn.tree import DecisionTreeClassifier
  tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
  tree.fit(X_train, Y_train)

  #Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm
  from sklearn.ensemble import RandomForestClassifier
  forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
  forest.fit(X_train, Y_train)
  
  #print model accuracy on the training data.
  print('[0]Logistic Regression Precisión:', log.score(X_train, Y_train))
  print('[1]K Nearest Neighbor Precisión:', knn.score(X_train, Y_train))
  print('[2]Support Vector Machine (Linear Classifier) Precisión:', svc_lin.score(X_train, Y_train))
  print('[3]Support Vector Machine (RBF Classifier) Precisión:', svc_rbf.score(X_train, Y_train))
  print('[4]Gaussian Naive Bayes Precisión:', gauss.score(X_train, Y_train))
  print('[5]Decision Tree Classifier Precisión:', tree.score(X_train, Y_train))
  print('[6]Random Forest Classifier Precisión:', forest.score(X_train, Y_train))
  
  return log, knn, svc_lin, svc_rbf, gauss, tree, forest

#Crearmos el modelo y observamos la puntuación de precisión.
model = models(X_train,Y_train)

#Construimos matriz confusión 
from sklearn.metrics import confusion_matrix
for i in range(len(model)):
  cm = confusion_matrix(Y_test, model[i].predict(X_test))
  
  TN = cm[0][0]
  TP = cm[1][1]
  FN = cm[1][0]
  FP = cm[0][1]
  
  print('Matriz de confusión')
  print(cm)
  print('Modelo[{}] Precisión = "{}!"'.format(i,  (TP + TN) / (TP + TN + FN + FP)))
  print()

#Muestre otras formas de obtener la precisión de la clasificación y otras métricas

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score

for i in range(len(model)):
  print('Model ',i)
  #Comprobar precisión, recordar, puntuación f1
  print( classification_report(Y_test, model[i].predict(X_test)) )
  #Otra forma de obtener la precisión de los modelos en los datos de prueba
  print( accuracy_score(Y_test, model[i].predict(X_test)))
  print()#Imprime una nueva línea

# Imprimir predicción del modelo de clasificador de bosque aleatorio
pred = model[6].predict(X_test)
print(pred)

#Imprimir un espacio
print()

#Imprima los valores reales
print(Y_test)